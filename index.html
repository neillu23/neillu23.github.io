<!DOCTYPE HTML>
<!--
	Personal Website
	ylu125 at jhu.edu 
	Ph.D. student |  CLSP
	Johns Hopkins University 
-->

<!-- #navlist li
{
display: inline;
list-style-type: none;
padding-right: 20px;
} -->

<html>
	
	
	<head>
		<title> Yen-Ju Lu </title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">

		<!-- Header -->
			<header id="header">
				<div class="inner">
					<a href="#" class="w3-circle"><img src="images/IMG_1885.jpg" width="230px" alt="" /></a>
					<h1><strong> Yen-Ju Lu </strong> <br />
					ylu125 at jhu.edu <br />
					Ph.D. student |  CLSP <br />
					Johns Hopkins University <br />
					Speech and Language Processing | Machine Learning </h1>
				</div>
			</header>

		<!-- Main -->
			<div id="main">

				<!-- One -->
					<section id="one">
						<header class="major">
							<h2> ABOUT ME </h2>
						</header>
						<p> I am a Ph.D. student in <a href="https://www.clsp.jhu.edu/"> Center for Language and Speech Processing</a>   at Johns Hopkins University, advised by Prof. Najim Dehak and Prof. Jesús Villalba. My research focuses on multimodal language models that integrate speech and text for real-world content understanding and generation by capturing paralinguistic cues with advanced encoders. I have also explored techniques to optimize large language models (LLMs) for more accurate and informative dialogue summarization.</p>
						<ul id="navlist">
							<li><a href="https://github.com/neillu23/neillu23.github.io/blob/main/images/Yen-Ju-Lu-CV.pdf" class="button">Learn More</a></li>
							<li><a href="https://scholar.google.com/citations?hl=en&view_op=list_works&gmla=AJsN-F6alvtcSjeve2WZxtMNZwAb3xgrCkTWA4y08BB8CFA5bBV-5pyT7Xj2iHahmbXKAu5NVRn7VDCkdinXmIAsgEpNHPbYdg&user=emtNw84AAAAJ" class="button">Google Scholar</a></li>
						</ul>
					</section>
				
					<section id="news">	
							<h2> NEWS </h2>
						<ul>
						    <li>
						      My internship project, "Mutual Reinforcement of LLM Dialogue Synthesis and Summarization Capabilities for Few-Shot Dialogue Summarization" is accepted by Findings of NAACL 2025!
						    </li>
						    <li>
						      Our recent work with Amazon, "CA-SSLR: Condition-Aware Self-Supervised Learning Representation for Generalized Speech Processing" is accepted by NeurIPS 2024!
						    </li>
						</ul> 
					</section>

				    <section id="experience">
				      <h2>Current Roles &amp; Recent Experiences</h2>
				      <ul>
					<li>
					      <strong>Upcoming Research Scientist Intern at Meta Generative AI (May 2025 – Aug 2025):</strong>
					      Collaborating on multimodal LLM projects to further advance language and speech integration.
				        </li>
					<li>
					      <strong>Research Scientist Intern at Apple Machine Learning Research (May 2024 – Aug 2024):</strong>
					       <br>
					      <em>Mentored by Ting-Yao Hu, Hema Swetha Koppula, Raviteja Vemulapalli and Oncel Tuzel</em>
					      <br>
						Developed  Mutual Reinforcing Data Synthesis (MRDS) within LLMs to improve few-shot dialogue summarization, accepted by Findings of NAACL 2025.
					</li>
					<li>
					      <strong>Amazon Mentorship Program, AI2AI (Sep. 2022 – May 2024):</strong>
						<br>
					      <em>Mentored by Jing Liu and Ariya Rastrow</em>
					      <br>
						Designed a condition-aware SSLR (CA-SSLR) model for generalist speech processing, accepted by NeurIPS 2024.
					</li>
					<li>
					  <strong>Cross-Institutional Collaboration:</strong> 
					  Led a project between Johns Hopkins University and Northwestern University, achieving notable improvements in model adaptation and quantization.
					</li>
				      </ul>
				    </section>
				<!-- Two -->
					<section id="two">
						<h2>Recent Works</h2>
						<div class="row">
							
							<article class="col-6 col-12-xsmall work-item">
 								<a href="https://arxiv.org/pdf/2502.17328" target="_blank" rel="noopener noreferrer">
									<img src="images/MRDS.png"  width="100%" alt="" /></a>
								<h3>Mutual Reinforcement of LLM Dialogue Synthesis and Summarization Capabilities for Few-Shot Dialogue Summarization (Findings of <strong>NAACL 2025</strong>) </h3>
								<p style="font-size:16px"> <strong>Yen-Ju Lu</strong>,Ting-Yao Hu, Hema Swetha Koppula, Hadi Pouransari, Jen-Hao Rick Chang, Yin Xia, Xiang Kong, Qi Zhu, Simon Wang, Oncel Tuzel, Raviteja Vemulapalli</p>
								<p> In this work, we propose Mutual Reinforcing Data Synthesis (MRDS) within LLMs to improve few-shot dialogue summarization task. Unlike prior methods that require external knowledge, we mutually reinforce the LLM\'s dialogue synthesis and summarization capabilities, allowing them to complement each other during training and enhance overall performances. The dialogue synthesis capability is enhanced by directed preference optimization with preference scoring from summarization capability. The summarization capability is enhanced by the additional high quality dialogue-summary paired data produced by the dialogue synthesis capability. By leveraging the proposed MRDS mechanism, we elicit the internal knowledge of LLM in the format of synthetic data, and use it to augment the few-shot real training dataset. </p>
								<ul class="actions">
									<li><a href="https://arxiv.org/pdf/2502.17328" class="button">Continue Reading</a></li>
								</ul>
							
							</article>

							<article class="col-6 col-12-xsmall work-item">
 								<a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/59a9cc95f046e9125d8816ef971873e7-Paper-Conference.pdf" target="_blank" rel="noopener noreferrer">
									<img src="images/CA-SSLR.png"  width="100%" alt="" /></a>
								<h3>CA-SSLR: Condition-aware self-supervised learning representation for generalized speech processing (<strong>NeurIPS 2024</strong>) </h3>
								<p style="font-size:16px"> <strong>Yen-Ju Lu</strong>, Jing Liu, Thomas Thebaud, Laureano Moro-Velazquez, Ariya Rastrow, Najim Dehak, Jesus Villalba</p>
								<p> We introduce Condition-Aware Self-Supervised Learning Representation (CA-SSLR), a generalist conditioning model broadly applicable to various speech-processing tasks. Compared to standard fine-tuning methods that optimize for downstream models, CA-SSLR integrates language and speaker embeddings from earlier layers, making the SSL model aware of the current language and speaker context. This approach reduces the reliance on the input audio features while preserving the integrity of the base SSLR. CA-SSLR improves the model’s capabilities and demonstrates its generality on unseen tasks with minimal task-specific tuning. </p>
								<ul class="actions">
									<li><a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/59a9cc95f046e9125d8816ef971873e7-Paper-Conference.pdf" class="button">Continue Reading</a></li>
								</ul>
							
							</article>

							
							<article class="col-6 col-12-xsmall work-item">
								<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10164201" target="_blank" rel="noopener noreferrer"><img src="images/TASLP.png"  width="100%" alt="" /></a>
								<h3> Improving Speech Enhancement Performance by Leveraging Contextual Broad Phonetic Class Information  <br> (<strong> TASLP 2023 </strong>) </h3>
								<p style="font-size:16px"> <strong>Yen-Ju Lu</strong>, Chia-Yu Chang, Cheng Yu,  Ching-Feng Liu,  Jeih-weih Hung, Shinji Watanabe, Yu Tsao </p>
								<p> In this article, we explore the contextual information of articulatory attributes as additional information to further benefit SE. More specifically, we propose to improve the SE performance by leveraging losses from an end-to-end automatic speech recognition (E2E-ASR) model that predicts the sequence of broad phonetic classes (BPCs). We also developed multi-objective training with ASR and perceptual losses to train the SE system based on a BPC-based E2E-ASR.  </p>
								<ul class="actions">
									<li><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10164201" class="button">Continue Reading</a></li>
								</ul>
							</article>
							



							
							<article class="col-6 col-12-xsmall work-item">
 								<a href="https://arxiv.org/abs/2207.09514" target="_blank" rel="noopener noreferrer">
									<img src="images/ESPnet-SE++.png"  width="100%" alt="" /></a>
								<h3> ESPnet-SE++: Speech Enhancement for Robust Speech Recognition, Translation, and Understanding. (<strong>Interspeech 2022</strong>) </h3>
								<p style="font-size:16px"> <strong>Yen-Ju Lu</strong>, Xuankai Chang, Chenda Li, Wangyou Zhang, Samuele Cornell, Zhaoheng Ni, Yoshiki Masuyama, Brian Yan, Robin Scheibler, Zhong-Qiu Wang, Yu Tsao, Yanmin Qian, Shinji Watanabe</p>
								<p> This paper presents recent progress on integrating speech separation and enhancement (SSE) into the ESPnet toolkit. Compared with the previous ESPnet-SE work, numerous features have been added, including recent state-of-the-art speech enhancement models with their respective training and evaluation recipes. Importantly, a new interface has been designed to flexibly combine speech enhancement front-ends with other tasks, including automatic speech recognition (ASR), speech translation (ST), and spoken language understanding (SLU). To showcase such integration, we performed experiments on carefully designed synthetic datasets for noisy-reverberant multi-channel ST and SLU tasks, which can be used as benchmark corpora for future research. In addition to these new tasks, we also use CHiME-4 and WSJ0-2Mix to benchmark multi- and single-channel SE approaches. Results show that the integration of SE front-ends with back-end tasks is a promising research direction even for tasks besides ASR, especially in the multi-channel scenario. The code is available online at this https URL. The multi-channel ST and SLU datasets, which are another contribution of this work, are released on HuggingFace. </p>
								<ul class="actions">
									<li><a href="https://arxiv.org/abs/2207.09514" class="button">Continue Reading</a></li>
								</ul>
							
							</article>
						
							<article class="col-6 col-12-xsmall work-item">
 								<a href="https://ieeexplore.ieee.org/abstract/document/9747146/" target="_blank" rel="noopener noreferrer">
									<img src="images/iNeuBe.png"  width="100%" alt="" /></a>
								<h3> Towards Low-distortion Multi-channel Speech Enhancement: The ESPNet-SE Submission to The L3DAS22 Challenge. (<strong>ICASSP 2022</strong>) </h3>
								<p style="font-size:16px"> <strong>Yen-Ju Lu</strong>, Samuele Cornell, Xuankai Chang, Wangyou Zhang, Chenda Li, Zhaoheng Ni, Zhong-Qiu Wang, Shinji Watanabe</p>
								<p> This paper describes our submission to the L3DAS22 Challenge Task 1, which consists of speech enhancement with 3D Ambisonic microphones. The core of our approach combines Deep Neural Network (DNN) driven complex spectral mapping with linear beamformers such as the multi-frame multi-channel Wiener filter. Our proposed system has two DNNs and a linear beamformer in between. Both DNNs are trained to perform complex spectral mapping, using a combination of waveform and magnitude spectrum losses. The estimated signal from the first DNN is used to drive a linear beamformer, and the beamforming result, together with this enhanced signal, are used as extra inputs for the second DNN which refines the estimation. Then, from this new estimated signal, the linear beamformer and second DNN are run iteratively. The proposed method was ranked first in the challenge, achieving, on the evaluation set, a ranking metric of 0.984, versus 0.833 of the challenge baseline. </p>
								<ul class="actions">
									<li><a href="https://ieeexplore.ieee.org/abstract/document/9747146/" class="button">Continue Reading</a></li>
								</ul>
							
							</article>
						
							<article class="col-6 col-12-xsmall work-item">
								<a href="https://arxiv.org/abs/2202.05256 target="_blank" rel="noopener noreferrer"><img src="images/CDiffuSE.png" width="100%" alt="" /></a>
								<h3> Conditional Diffusion Probabilistic Model for Speech Enhancement. (<strong>ICASSP 2022</strong>)  </h3>
								<p style="font-size:16px"> <strong>Yen-Ju Lu </strong> , Zhong-Qiu Wang, Shinji Watanabe, Alexander Richard, Cheng Yu, Yu Tsao</p>
								<p> Speech enhancement is a critical component of many user-oriented audio applications, yet current systems still suffer from distorted and unnatural outputs. While generative models have shown strong potential in speech synthesis, they are still lagging behind in speech enhancement. This work leverages recent advances in diffusion probabilistic models, and proposes a novel speech enhancement algorithm that incorporates characteristics of the observed noisy speech signal into the diffusion and reverse processes. More specifically, we propose a generalized formulation of the diffusion probabilistic model named conditional diffusion probabilistic model that, in its reverse process, can adapt to non-Gaussian real noises in the estimated speech signal. In our experiments, we demonstrate strong performance of the proposed approach compared to representative generative models, and investigate the generalization capability of our models to other datasets with noise characteristics unseen during training. </p>
								<ul class="actions">
									<li><a href="https://arxiv.org/abs/2202.05256" class="button">Continue Reading</a></li>
								</ul>
							</article>	
						
							<article class="col-6 col-12-xsmall work-item">
								<a href="https://arxiv.org/abs/2107.11876" target="_blank" rel="noopener noreferrer"><img src="images/DiffuSE.png" width="100%" alt="" /></a>
								<h3> A Study on Speech Enhancement Based on Diffusion Probabilistic Model. (<strong>APSIPA 2021</strong>) </h3>
								<p style="font-size:16px"> <strong>Yen-Ju Lu </strong> , Yu Tsao, Shinji Watanabe</p>
								<p> Diffusion probabilistic models have demonstrated an outstanding capability to model natural images and raw audio waveforms through a paired diffusion and reverse processes. The unique property of the reverse process (namely, eliminating non-target signals from the Gaussian noise and noisy signals) could be utilized to restore clean signals. Based on this property, we propose a diffusion probabilistic model-based speech enhancement (DiffuSE). To attain better enhancement performance, we designed an advanced reverse process, termed the supportive reverse process, which adds noisy speech in each time-step to the predicted speech. The experimental results show that DiffuSE yields performance that is comparable to related audio generative models on the standardized Voice Bank corpus SE task. Moreover, relative to the generally suggested full sampling schedule, the proposed supportive reverse process especially improved the fast sampling, taking few steps to yield better enhancement results over the conventional full step inference process. </p>
								<ul class="actions">
									<li><a href="https://arxiv.org/abs/2107.11876" class="button">Continue Reading</a></li>
								</ul>
							</article>
							
							<article class="col-6 col-12-xsmall work-item">
<!-- 								<a href="https://asru2021.org/" target="_blank" rel="noopener noreferrer"> -->
<!-- 									<img src="images/S3PRL.png"  width="100%" alt="" /></a> -->
								<a href="https://arxiv.org/abs/2110.04590" target="_blank" rel="noopener noreferrer"><img src="images/S3PRL.png"  width="100%" alt="" /></a>
								<h3> An Exploration of Self-Supervised Pretrained Representations for End-To-End Speech Recognition. (<strong>ASRU 2021</strong>) </h3>
								<p style="font-size:16px"> Xuankai Chang, Takashi Maekaku, Pengcheng Guo, Jing Shi, <strong>Yen-Ju Lu</strong>, Aswin Shanmugam Subramanian, Tianzi Wang, Shu-wen Yang, Yu Tsao, Hung-yi Lee, Shinji Watanabe </p>
								<p> Self-supervised pretraining on speech data has achieved a lot of progress. High-fidelity representation of the speech signal is learned from a lot of untranscribed data and shows promising performance. In this paper, we focus on the general applications of pretrained speech representations, on advanced end-to-end automatic speech recognition (S-ASR) models. We select several pretrained speech representations and present the experimental results on various open-source and publicly available corpora for E2E-ASR. </p> 
								<ul class="actions">
									<li><a href="https://arxiv.org/abs/2110.04590" class="button">Continue Reading</a></li>
								</ul>

							</article>
							
							<article class="col-6 col-12-xsmall work-item">
								<a href="https://www.isca-speech.org/archive/pdfs/interspeech_2021/lin21c_interspeech.pdf" target="_blank" rel="noopener noreferrer"><img src="images/Qista.png" width="100%" alt="" /></a>
								<h3> QISTA-Net-Audio: Audio Super-resolution via Non-Convex l<sub>q</sub>-norm Minimization. (<strong>Interspeech 2021</strong>) </h3>
								<p style="font-size:16px"> Gang-Xuan Lin, Shih-Wei Hu, <strong>Yen-Ju Lu</strong>, Yu Tsao, Chun-Shien Lu </p>
								<p> In this paper, we propose a learning model, QISTA-Net-Audio, to solve ASR in a paradigm of linear inverse problem. QISTA-Net-Audio is composed of two components. First, an audio waveform can be presented as a complex-valued spectrum, which is composed of a real and an imaginary part, in the frequency domain. We treat the real and imaginary parts as an image, and predict a high-resolution spectrum but only keep the phase information from the viewpoint of image reconstruction. Second, we predict the magnitude information by solving the sparse signal reconstruction problem. By combining the predicted magnitude and the phase together, we can recover the high-resolution waveform. </p>
								<ul class="actions">
									<li><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2021/lin21c_interspeech.pdf" class="button">Continue Reading</a></li>
								</ul>
							</article>
				
							<article class="col-6 col-12-xsmall work-item">
								<a href="https://www.isca-speech.org/archive_v0/Interspeech_2020/pdfs/1400.pdf" target="_blank" rel="noopener noreferrer"><img src="images/BPC.png"  width="100%" alt="" /></a>
								<h3> Incorporating Broad Phonetic Information for Speech Enhancement. (<strong>Interspeech 2020</strong>) </h3>
								<p style="font-size:16px"> <strong>Yen-Ju Lu</strong>, Chien-Feng Liao, Xugang Lu, Jeih-Weih Hung, Yu Tsao </p>
								<p> We have investigated three criteria to build the BPC, including two knowledgebased criteria: place and manner of articulatory and one datadriven criterion. Moreover, the recognition accuracies of BPCs are much higher than that of phonemes, thus providing more accurate phonetic information to guide the SE process under very noisy conditions. Experimental results demonstrate that the proposed SE with the BPC information framework can achieve notable performance improvements over the baseline system and an SE system using monophonic information in term </p>
								<ul class="actions">
									<li><a href="https://www.isca-speech.org/archive_v0/Interspeech_2020/pdfs/1400.pdf" class="button">Continue Reading</a></li>
								</ul>
							</article>
							
							<article class="col-6 col-12-xsmall work-item">
								<a href="http://www.apsipa.org/proceedings/2020/pdfs/0000455.pdf" target="_blank" rel="noopener noreferrer"><img src="images/boost.png"  width="40%" alt="" /></a>
								<h3> Boosting Objective Scores of a Speech Enhancement Model by MetricGAN Post-processing (<strong>APSIPA 2020</strong>)</h3>
								<p style="font-size:16px"> Szu-Wei Fu, Chien-Feng Liao, Tsun-An Hsieh, Kuo-Hsuan Hung, Syu-Siang Wang, Cheng Yu, HengCheng Kuo, Ryandhimas E. Zezario, You-Jin Li, Shang-Yi Chuang, <strong>Yen-Ju Lu</strong>, Yu-Chen Lin ,Yu Tsao</p>
								<p> The Transformer architecture has demonstrated a superior ability compared to recurrent neural networks in many different natural language processing applications. Therefore, our study applies a modified Transformer in a speech enhancement task. Specifically, positional encoding in the Transformer may not be necessary for speech enhancement, and hence, it is replaced by convolutional layers. To further improve the perceptual evaluation of the speech quality (PESQ) scores of enhanced speech, the 𝑳𝟏 pre-trained Transformer is fine-tuned using a MetricGAN framework.  </p>
								<ul class="actions">
									<li><a href="http://www.apsipa.org/proceedings/2020/pdfs/0000455.pdf" class="button">Continue Reading</a></li>
								</ul>
							</article>
						</div>
						<ul class="actions">
							<li><a href="https://github.com/neillu23/neillu23.github.io/blob/main/images/Yen-Ju-Lu-CV.pdf" class="button">Full Portfolio</a></li>
<!-- 							<li><a href="#" class="button">Full Portfolio</a></li> -->
						</ul>
					</section>

 				<!-- Three -->
<!-- 					<section id="three">
						<h2>Get In Touch</h2>
						<p>Accumsan pellentesque commodo blandit enim arcu non at amet id arcu magna. Accumsan orci faucibus id eu lorem semper nunc nisi lorem vulputate lorem neque lorem ipsum dolor.</p>
						<div class="row">
							<div class="col-8 col-12-small">
								<form method="post" action="#">
									<div class="row gtr-uniform gtr-50">
										<div class="col-6 col-12-xsmall"><input type="text" name="name" id="name" placeholder="Name" /></div>
										<div class="col-6 col-12-xsmall"><input type="email" name="email" id="email" placeholder="Email" /></div>
										<div class="col-12"><textarea name="message" id="message" placeholder="Message" rows="4"></textarea></div>
									</div>
								</form>
								<ul class="actions">
									<li><input type="submit" value="Send Message" /></li>
								</ul>
							</div>
							<div class="col-4 col-12-small">
								<ul class="labeled-icons">
									<li>
										<h3 class="icon solid fa-home"><span class="label">Address</span></h3>
										1234 Somewhere Rd.<br />
										Nashville, TN 00000<br />
										United States
									</li>
									<li>
										<h3 class="icon solid fa-mobile-alt"><span class="label">Phone</span></h3>
										000-000-0000
									</li>
									<li>
										<h3 class="icon solid fa-envelope"><span class="label">Email</span></h3>
										<a href="#">hello@untitled.tld</a>
									</li>
								</ul>
							</div>
						</div>
					</section> -->

				<!-- Four -->
				<!--
					<section id="four">
						<h2>Elements</h2>

						<section>
							<h4>Text</h4>
							<p>This is <b>bold</b> and this is <strong>strong</strong>. This is <i>italic</i> and this is <em>emphasized</em>.
							This is <sup>superscript</sup> text and this is <sub>subscript</sub> text.
							This is <u>underlined</u> and this is code: <code>for (;;) { ... }</code>. Finally, <a href="#">this is a link</a>.</p>
							<hr />
							<header>
								<h4>Heading with a Subtitle</h4>
								<p>Lorem ipsum dolor sit amet nullam id egestas urna aliquam</p>
							</header>
							<p>Nunc lacinia ante nunc ac lobortis. Interdum adipiscing gravida odio porttitor sem non mi integer non faucibus ornare mi ut ante amet placerat aliquet. Volutpat eu sed ante lacinia sapien lorem accumsan varius montes viverra nibh in adipiscing blandit tempus accumsan.</p>
							<header>
								<h5>Heading with a Subtitle</h5>
								<p>Lorem ipsum dolor sit amet nullam id egestas urna aliquam</p>
							</header>
							<p>Nunc lacinia ante nunc ac lobortis. Interdum adipiscing gravida odio porttitor sem non mi integer non faucibus ornare mi ut ante amet placerat aliquet. Volutpat eu sed ante lacinia sapien lorem accumsan varius montes viverra nibh in adipiscing blandit tempus accumsan.</p>
							<hr />
							<h2>Heading Level 2</h2>
							<h3>Heading Level 3</h3>
							<h4>Heading Level 4</h4>
							<h5>Heading Level 5</h5>
							<h6>Heading Level 6</h6>
							<hr />
							<h5>Blockquote</h5>
							<blockquote>Fringilla nisl. Donec accumsan interdum nisi, quis tincidunt felis sagittis eget tempus euismod. Vestibulum ante ipsum primis in faucibus vestibulum. Blandit adipiscing eu felis iaculis volutpat ac adipiscing accumsan faucibus. Vestibulum ante ipsum primis in faucibus lorem ipsum dolor sit amet nullam adipiscing eu felis.</blockquote>
							<h5>Preformatted</h5>
							<pre><code>i = 0;

while (!deck.isInOrder()) {
print 'Iteration ' + i;
deck.shuffle();
i++;
}

print 'It took ' + i + ' iterations to sort the deck.';</code></pre>
						</section>

						<section>
							<h4>Lists</h4>
							<div class="row">
								<div class="col-6 col-12-xsmall">
									<h5>Unordered</h5>
									<ul>
										<li>Dolor pulvinar etiam magna etiam.</li>
										<li>Sagittis adipiscing lorem eleifend.</li>
										<li>Felis enim feugiat dolore viverra.</li>
									</ul>
									<h5>Alternate</h5>
									<ul class="alt">
										<li>Dolor pulvinar etiam magna etiam.</li>
										<li>Sagittis adipiscing lorem eleifend.</li>
										<li>Felis enim feugiat dolore viverra.</li>
									</ul>
								</div>
								<div class="col-6 col-12-xsmall">
									<h5>Ordered</h5>
									<ol>
										<li>Dolor pulvinar etiam magna etiam.</li>
										<li>Etiam vel felis at lorem sed viverra.</li>
										<li>Felis enim feugiat dolore viverra.</li>
										<li>Dolor pulvinar etiam magna etiam.</li>
										<li>Etiam vel felis at lorem sed viverra.</li>
										<li>Felis enim feugiat dolore viverra.</li>
									</ol>
									<h5>Icons</h5>
									<ul class="icons">
										<li><a href="#" class="icon brands fa-twitter"><span class="label">Twitter</span></a></li>
										<li><a href="#" class="icon brands fa-facebook-f"><span class="label">Facebook</span></a></li>
										<li><a href="#" class="icon brands fa-instagram"><span class="label">Instagram</span></a></li>
										<li><a href="#" class="icon brands fa-github"><span class="label">Github</span></a></li>
										<li><a href="#" class="icon brands fa-dribbble"><span class="label">Dribbble</span></a></li>
										<li><a href="#" class="icon brands fa-tumblr"><span class="label">Tumblr</span></a></li>
									</ul>
								</div>
							</div>
							<h5>Actions</h5>
							<ul class="actions">
								<li><a href="#" class="button primary">Default</a></li>
								<li><a href="#" class="button">Default</a></li>
							</ul>
							<ul class="actions small">
								<li><a href="#" class="button primary small">Small</a></li>
								<li><a href="#" class="button small">Small</a></li>
							</ul>
							<div class="row">
								<div class="col-6 col-12-small">
									<ul class="actions stacked">
										<li><a href="#" class="button primary">Default</a></li>
										<li><a href="#" class="button">Default</a></li>
									</ul>
								</div>
								<div class="col-6 col-12-small">
									<ul class="actions stacked">
										<li><a href="#" class="button primary small">Small</a></li>
										<li><a href="#" class="button small">Small</a></li>
									</ul>
								</div>
								<div class="col-6 col-12-small">
									<ul class="actions stacked">
										<li><a href="#" class="button primary fit">Default</a></li>
										<li><a href="#" class="button fit">Default</a></li>
									</ul>
								</div>
								<div class="col-6 col-12-small">
									<ul class="actions stacked">
										<li><a href="#" class="button primary small fit">Small</a></li>
										<li><a href="#" class="button small fit">Small</a></li>
									</ul>
								</div>
							</div>
						</section>

						<section>
							<h4>Table</h4>
							<h5>Default</h5>
							<div class="table-wrapper">
								<table>
									<thead>
										<tr>
											<th>Name</th>
											<th>Description</th>
											<th>Price</th>
										</tr>
									</thead>
									<tbody>
										<tr>
											<td>Item One</td>
											<td>Ante turpis integer aliquet porttitor.</td>
											<td>29.99</td>
										</tr>
										<tr>
											<td>Item Two</td>
											<td>Vis ac commodo adipiscing arcu aliquet.</td>
											<td>19.99</td>
										</tr>
										<tr>
											<td>Item Three</td>
											<td> Morbi faucibus arcu accumsan lorem.</td>
											<td>29.99</td>
										</tr>
										<tr>
											<td>Item Four</td>
											<td>Vitae integer tempus condimentum.</td>
											<td>19.99</td>
										</tr>
										<tr>
											<td>Item Five</td>
											<td>Ante turpis integer aliquet porttitor.</td>
											<td>29.99</td>
										</tr>
									</tbody>
									<tfoot>
										<tr>
											<td colspan="2"></td>
											<td>100.00</td>
										</tr>
									</tfoot>
								</table>
							</div>

							<h5>Alternate</h5>
							<div class="table-wrapper">
								<table class="alt">
									<thead>
										<tr>
											<th>Name</th>
											<th>Description</th>
											<th>Price</th>
										</tr>
									</thead>
									<tbody>
										<tr>
											<td>Item One</td>
											<td>Ante turpis integer aliquet porttitor.</td>
											<td>29.99</td>
										</tr>
										<tr>
											<td>Item Two</td>
											<td>Vis ac commodo adipiscing arcu aliquet.</td>
											<td>19.99</td>
										</tr>
										<tr>
											<td>Item Three</td>
											<td> Morbi faucibus arcu accumsan lorem.</td>
											<td>29.99</td>
										</tr>
										<tr>
											<td>Item Four</td>
											<td>Vitae integer tempus condimentum.</td>
											<td>19.99</td>
										</tr>
										<tr>
											<td>Item Five</td>
											<td>Ante turpis integer aliquet porttitor.</td>
											<td>29.99</td>
										</tr>
									</tbody>
									<tfoot>
										<tr>
											<td colspan="2"></td>
											<td>100.00</td>
										</tr>
									</tfoot>
								</table>
							</div>
						</section>

						<section>
							<h4>Buttons</h4>
							<ul class="actions">
								<li><a href="#" class="button primary">Primary</a></li>
								<li><a href="#" class="button">Default</a></li>
							</ul>
							<ul class="actions">
								<li><a href="#" class="button large">Large</a></li>
								<li><a href="#" class="button">Default</a></li>
								<li><a href="#" class="button small">Small</a></li>
							</ul>
							<ul class="actions fit">
								<li><a href="#" class="button primary fit">Fit</a></li>
								<li><a href="#" class="button fit">Fit</a></li>
							</ul>
							<ul class="actions fit small">
								<li><a href="#" class="button primary fit small">Fit + Small</a></li>
								<li><a href="#" class="button fit small">Fit + Small</a></li>
							</ul>
							<ul class="actions">
								<li><a href="#" class="button primary icon solid fa-download">Icon</a></li>
								<li><a href="#" class="button icon solid fa-download">Icon</a></li>
							</ul>
							<ul class="actions">
								<li><span class="button primary disabled">Primary</span></li>
								<li><span class="button disabled">Default</span></li>
							</ul>
						</section>

						<section>
							<h4>Form</h4>
							<form method="post" action="#">
								<div class="row gtr-uniform gtr-50">
									<div class="col-6 col-12-xsmall">
										<input type="text" name="demo-name" id="demo-name" value="" placeholder="Name" />
									</div>
									<div class="col-6 col-12-xsmall">
										<input type="email" name="demo-email" id="demo-email" value="" placeholder="Email" />
									</div>
									<div class="col-12">
										<select name="demo-category" id="demo-category">
											<option value="">- Category -</option>
											<option value="1">Manufacturing</option>
											<option value="1">Shipping</option>
											<option value="1">Administration</option>
											<option value="1">Human Resources</option>
										</select>
									</div>
									<div class="col-4 col-12-small">
										<input type="radio" id="demo-priority-low" name="demo-priority" checked>
										<label for="demo-priority-low">Low Priority</label>
									</div>
									<div class="col-4 col-12-small">
										<input type="radio" id="demo-priority-normal" name="demo-priority">
										<label for="demo-priority-normal">Normal Priority</label>
									</div>
									<div class="col-4 col-12-small">
										<input type="radio" id="demo-priority-high" name="demo-priority">
										<label for="demo-priority-high">High Priority</label>
									</div>
									<div class="col-6 col-12-small">
										<input type="checkbox" id="demo-copy" name="demo-copy">
										<label for="demo-copy">Email me a copy of this message</label>
									</div>
									<div class="col-6 col-12-small">
										<input type="checkbox" id="demo-human" name="demo-human" checked>
										<label for="demo-human">I am a human and not a robot</label>
									</div>
									<div class="col-12">
										<textarea name="demo-message" id="demo-message" placeholder="Enter your message" rows="6"></textarea>
									</div>
									<div class="col-12">
										<ul class="actions">
											<li><input type="submit" value="Send Message" class="primary" /></li>
											<li><input type="reset" value="Reset" /></li>
										</ul>
									</div>
								</div>
							</form>
						</section>

						<section>
							<h4>Image</h4>
							<h5>Fit</h5>
							<div class="box alt">
								<div class="row gtr-50 gtr-uniform">
									<div class="col-12"><span class="image fit"><img src="images/fulls/05.jpg" alt="" /></span></div>
									<div class="col-4"><span class="image fit"><img src="images/thumbs/01.jpg" alt="" /></span></div>
									<div class="col-4"><span class="image fit"><img src="images/thumbs/02.jpg" alt="" /></span></div>
									<div class="col-4"><span class="image fit"><img src="images/thumbs/03.jpg" alt="" /></span></div>
									<div class="col-4"><span class="image fit"><img src="images/thumbs/04.jpg" alt="" /></span></div>
									<div class="col-4"><span class="image fit"><img src="images/thumbs/05.jpg" alt="" /></span></div>
									<div class="col-4"><span class="image fit"><img src="images/thumbs/06.jpg" alt="" /></span></div>
									<div class="col-4"><span class="image fit"><img src="images/thumbs/03.jpg" alt="" /></span></div>
									<div class="col-4"><span class="image fit"><img src="images/thumbs/02.jpg" alt="" /></span></div>
									<div class="col-4"><span class="image fit"><img src="images/thumbs/01.jpg" alt="" /></span></div>
								</div>
							</div>
							<h5>Left &amp; Right</h5>
							<p><span class="image left"><img src="images/avatar.jpg" alt="" /></span>Fringilla nisl. Donec accumsan interdum nisi, quis tincidunt felis sagittis eget. tempus euismod. Vestibulum ante ipsum primis in faucibus vestibulum. Blandit adipiscing eu felis iaculis volutpat ac adipiscing accumsan eu faucibus. Integer ac pellentesque praesent tincidunt felis sagittis eget. tempus euismod. Vestibulum ante ipsum primis in faucibus vestibulum. Blandit adipiscing eu felis iaculis volutpat ac adipiscing accumsan eu faucibus. Integer ac pellentesque praesent. Donec accumsan interdum nisi, quis tincidunt felis sagittis eget. tempus euismod. Vestibulum ante ipsum primis in faucibus vestibulum. Blandit adipiscing eu felis iaculis volutpat ac adipiscing accumsan eu faucibus. Integer ac pellentesque praesent tincidunt felis sagittis eget. tempus euismod. Vestibulum ante ipsum primis in faucibus vestibulum. Blandit adipiscing eu felis iaculis volutpat ac adipiscing accumsan eu faucibus. Integer ac pellentesque praesent.</p>
							<p><span class="image right"><img src="images/avatar.jpg" alt="" /></span>Fringilla nisl. Donec accumsan interdum nisi, quis tincidunt felis sagittis eget. tempus euismod. Vestibulum ante ipsum primis in faucibus vestibulum. Blandit adipiscing eu felis iaculis volutpat ac adipiscing accumsan eu faucibus. Integer ac pellentesque praesent tincidunt felis sagittis eget. tempus euismod. Vestibulum ante ipsum primis in faucibus vestibulum. Blandit adipiscing eu felis iaculis volutpat ac adipiscing accumsan eu faucibus. Integer ac pellentesque praesent. Donec accumsan interdum nisi, quis tincidunt felis sagittis eget. tempus euismod. Vestibulum ante ipsum primis in faucibus vestibulum. Blandit adipiscing eu felis iaculis volutpat ac adipiscing accumsan eu faucibus. Integer ac pellentesque praesent tincidunt felis sagittis eget. tempus euismod. Vestibulum ante ipsum primis in faucibus vestibulum. Blandit adipiscing eu felis iaculis volutpat ac adipiscing accumsan eu faucibus. Integer ac pellentesque praesent.</p>
						</section>

					</section>
				-->

			</div>

		<!-- Footer -->
			<footer id="footer">
				<div class="inner">
					<ul class="icons">
						<li><a href="#" class="icon brands fa-twitter"><span class="label">Twitter</span></a></li>
						<li><a href="#" class="icon brands fa-github"><span class="label">Github</span></a></li>
						<li><a href="#" class="icon brands fa-dribbble"><span class="label">Dribbble</span></a></li>
						<li><a href="#" class="icon solid fa-envelope"><span class="label">Email</span></a></li>
					</ul>
					<ul class="copyright">
						<li>&copy; Untitled</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
					</ul>
				</div>
			</footer>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.poptrox.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
